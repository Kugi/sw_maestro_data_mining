{
 "metadata": {
  "name": "tutor_svm"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Machine Learning"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "* Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages. After learning, it can then be used to classify new email messages into spam and non-spam folders.\n",
      "* \uc8fc\uc5b4\uc9c4 example \uc5d0\uc11c \ud328\ud134\uc744 \uc778\uc2dd\ud574\uc11c \uc9c0\ub2a5\uc801\uc778 \uacb0\uc815\uc744 \ub0b4\ub9b0\ub2e4. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Algorithm types"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Supervised learning algorithms are trained on labelled examples, i.e., input where the desired output is known. The supervised learning algorithm attempts to generalise a function or mapping from inputs to outputs which can then be used to speculatively generate an output for previously unseen inputs.\n",
      " * classification \n",
      " * regression \n",
      "* Unsupervised learning algorithms operate on unlabelled examples, i.e., input where the desired output is unknown. Here the objective is to discover structure in the data (e.g. through a cluster analysis), not to generalise a mapping from inputs to outputs.\n",
      " * Clustering\n",
      "* Semi-supervised learning combines both labeled and unlabelled examples to generate an appropriate function or classifier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\uc2e4\uc2b5 \ud658\uacbd"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " * IPython : \ud604\uc7ac \ubcf4\uace0 \uc788\ub294 \ube0c\ub77c\uc6b0\uc800\uc0c1\uc5d0\uc11c python \uc744 \uc4f8\uc218 \uc788\uac8c \ud574\uc8fc\ub294 \uac1c\ubc1c \ud658\uacbd\uc774\ub2e4. Interactive Python \uc758 \uc57d\uc790\ub85c IPython \uc774\uace0, \ud604\uc7ac\ub294 \uc5f0\uad6c\uc6a9\uc774\ub098, \ud559\uc2b5\uc6a9\uc73c\ub85c \ud574\uc678\uc5d0\uc11c \ub110\ub9ac \uc0ac\uc6a9\ub418\uace0 \uc788\ub2e4. \ud574\uc678 \uc138\ubbf8\ub098 \ub3d9\uc601\uc0c1\ub4e4\uc744 \ubcf4\uba74 \ub300\ubd80\ubd84 IPython \uc73c\ub85c \ud558\uace0 \uc788\ub294\uac83\uc744 \ubcfc \uc218 \uc788\ub2e4. \uc608) https://us.pycon.org/2013/schedule/\n",
      "  * \ud639\uc2dc \uacf5\ubd80\ud558\uace0 \uc2f6\ub2e4\uba74 http://pyvideo.org/video/1652/ipython-in-depth-high-productivity-interactive-a-0 \uc774 \ub3d9\uc601\uc0c1\uc744 \ubcf4\uace0 \ub530\ub77c \ud574\ubcf4\uba74 \ub41c\ub2e4.\n",
      " * scikit : python \uc5d0\uc11c machine learning , data mining \uc744 \ud560\ub54c \ud544\uc218\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \uc624\ud508\uc18c\uc2a4\ub85c\uc11c, \uac70\uc758 \ub300\ubd80\ubd84\uc758 machine learning \uc54c\uace0\ub9ac\uc998\uc774 \uc788\uc744 \ubfd0\ub9cc \uc544\ub2c8\ub77c, \uadf8\uac83\uc744 \uc544\uc8fc \ud3b8\ud558\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \uc2dc\uc2a4\ud15c\ud654 \ub418\uc5b4 \uc788\ub2e4.\n",
      "  * scikit \uae30\ubcf8 : http://pyvideo.org/video/1655/an-introduction-to-scikit-learn-machine-learning\n",
      "  * scikit \uace0\uae09 : http://pyvideo.org/video/1719/advanced-machine-learning-with-scikit-learn\n",
      "                     \n",
      " * pandas, numpy : scikit \uc640 \ud568\uaed8 \ub9ce\uc774 \uc0ac\uc6a9\ub418\ub294 \uc624\ud508\uc18c\uc2a4\ub85c\uc11c, maching learning, data mining \ud560\ub54c \ud544\uc218\uc801\uc73c\ub85c \uc0ac\uc6a9\ub41c\ub2e4. \uc5ec\uae30 \uc6a9\ub3c4\uc5d0 \ub9de\uac8c \uc190\uc27d\uac8c \uc4f8 \uc218 \uc788\ub294 \ub9ce\uc778 \ub370\uc774\ud130 \ud615\ub4e4\uc774 \uc815\uc758 \ub418\uc5b4 \uc788\ub2e4.\n",
      " * wakario.io : \ud604\uc7ac \ubcf4\uace0 \uc788\ub294 \uc0ac\uc774\ud2b8. \uc704 Ipython, scikit, pandas, numpy \uc124\uc815\uc744 \ub85c\uceec \ucef4\ud4e8\ud130\uc5d0 \ud558\ub824\uba74 \uc2dc\uac04\uc774 \uc880 \uac78\ub9ac\ub294\ub370, \uc774 \ud658\uacbd\uc744 \ud074\ub77c\uc6b0\ub4dc \uc0c1\uc5d0\uc11c \uc81c\uacf5\ud574\uc8fc\ub294 \uc11c\ube44\uc2a4\uc774\ub2e4. wakari.io \ubb34\ub8cc \ubc84\uc804\uc5d0\uc11c \uc131\ub2a5\uc0c1\uc5d0 \uc81c\uc57d\uc774 \uc788\uc5b4\uc11c, \uc804\uccb4 \uc8fc\uc81c\uc640, \uc804\uccb4 \ubb38\uc11c\ub97c \ub300\uc0c1\uc73c\ub85c \uc2e4\ud5d8\uc744 \ud558\uae30\ub294 \uc5b4\ub835\ub2e4. \uadf8\ub798\uc11c \uc77c\ubd80 \uc120\ud0dd\ud55c 2\uac00\uc9c0 \uc8fc\uc81c\uc640, \uc18c\ub7c9\uc758 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \ud559\uc2b5 \ubc0f \uc131\ub2a5 \ud3c9\uac00\ub97c \uc218\ud589\ud55c\ub2e4. \ucd94\ud6c4\uc5d0 \uc81c\ub300\ub85c \ud574\ubcf4\uace0 \uc2f6\ub2e4\uba74 \ub85c\uceec\uc5d0\uc11c \ud658\uacbd\uc744 \uad6c\ucd95\ud55c \ub2e4\uc74c\uc5d0 \ud574\ubcf4\uba74 \ub41c\ub2e4.      \n",
      " * scrapy : python \uc5d0\uc11c \ud06c\ub864\ub9c1 \ud560\ub54c \uc0ac\uc6a9\ud558\ub294 \ub300\ud45c\uc801\uc778 \uc624\ud508\uc18c\uc2a4\ub85c\uc11c, \ud06c\ub864\ub9c1\uc744 \uc6b0\uc544\ud558\uace0 \uc27d\uac8c \ud560 \uc218 \uc788\uac8c \ub3c4\uc640\uc900\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame, Series\n",
      "import pandas as pd\n",
      "import pandas.io.sql as sql\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. \ud559\uc2b5 \ub370\uc774\ud130 \uad6c\ucd95 (Web Data Crawling)\n",
      " * \ud604\uc7ac \ud658\uacbd\uc5d0\uc11c scrapy \ub97c \uc124\uce58 \ud574\uc57c\ud55c\ub2e4. \n",
      " * \ud604\uc7ac \ube0c\ub77c\uc6b0\uc800 \uc0c1\ub2e8 Terminals \uc744 \uc120\ud0dd\ud6c4, Shell \uc744 \uc120\ud0dd\ud55c \ud6c4\uc5d0 +Tab \ubc84\ud2bc\uc744 \ub204\ub978\ub2e4. \uadf8\ub7ec\uba74 \ud130\ubbf8\ub110\uc774 \ub098\uc628\ub2e4.\n",
      " * \ub2e4\uc74c \uc124\uce58 \uba85\ub839\uc5b4\ub97c \uc785\ub825\ud55c\ub2e4. : pip install zope.interface scrapy\n",
      "  * scrapy \uc124\uce58\ud560\ub54c \uc5d0\ub7ec\uac00 \uc880 \ub098\uae34 \ud558\ub294\ub370, \ud574\uacb0\ud558\ub824\uba74 root \uad8c\ud55c\uc73c\ub85c \ub2e4\ub978 \ud328\ud0a4\uc9c0\ub4e4\uc744 \uc124\uce58\ud574\uc918\uc57c \ud55c\ub2e4. \uadf8\ub7f0\ub370 \uc5ec\uae30 wakario.io \uc5d0\uc11c \ub8e8\ud2b8 \uad8c\ud55c\uc744 \uc8fc\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \uc5ec\uae30\uc11c\ub294 \ud574\uacb0\uc774 \uc5b4\ub835\ub2e4. \uc544\ub798 \ucf54\ub4dc\ub4e4\uc744 \ub3cc\ub824\ubcf4\ub294\uac83\uacfc\ub294 \uc0c1\uad00 \uc5c6\ub294 \uc5d0\ub7ec\uc774\uae30 \ub54c\ubb38\uc5d0 \uc77c\ub2e8 \ubb34\uc2dc\ud574\ub3c4 \ub41c\ub2e4.\n",
      "  \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import scrapy.utils.markup as markup\n",
      "from scrapy.selector import HtmlXPathSelector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named scrapy.utils.markup",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-e23ecafe9258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHtmlXPathSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named scrapy.utils.markup"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re,requests\n",
      "page_info_re = re.compile(\"http://blog\\.naver\\.com/(?P<user_id>.*?)\\?Redirect=Log&logNo=(?P<post_id>.*?)&\")\n",
      "page_info_re2 = re.compile(\"http://(?P<user_id>.*?)\\.blog\\.me/(?P<post_id>.*?)$\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.1 \ub124\uc774\ubc84 \ube14\ub85c\uadf8 \uae00 \ucd94\ucd9c\uae30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_naver_blog_data(page_url):\n",
      "    post_info ={}\n",
      "    for find_re in [page_info_re,page_info_re2]:\n",
      "        find_match = find_re.match(page_url)\n",
      "        \n",
      "        if find_match:\n",
      "            post_info = find_match.groupdict()\n",
      "    if not post_info:\n",
      "        return \"\"\n",
      "    post_url = \"http://blog.naver.com/PostView.nhn?blogId=%s&logNo=%s\"%(post_info['user_id'],post_info['post_id'])\n",
      "    html_data = requests.get(post_url).text\n",
      "    #print html_data\n",
      "    try:\n",
      "        main_text = HtmlXPathSelector(text=html_data).select(\"//*[@id='post-view%s']\"%(post_info['post_id'])).extract()[0]\n",
      "    except Exception,e:\n",
      "        print \"exception in get data:\",e\n",
      "        return \"\"\n",
      "    #print \"D:\",mainframe_url\n",
      "    data = markup.remove_tags(main_text).strip()    \n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://blog.naver.com/ehdvkf380?Redirect=Log&logNo=20193683920&from=section'\n",
      "print get_naver_blog_data(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* \ub124\uc774\ubc84 \ube14\ub85c\uadf8\uc5d0 \uc788\ub294 \uce74\ud14c\uace0\ub9ac \ubaa9\ub85d\uc744 \uac00\uc838\uc628\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cate_xpath = \".//*[@name='directoryItem']\"\n",
      "url = \"http://section.blog.naver.com/sub/PostListByDirectory.nhn?option.directorySeq=5\"\n",
      "req = requests.get(url)\n",
      "hxs = HtmlXPathSelector(text=req.text)\n",
      "blog_cate_list = hxs.select(cate_xpath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* \uac01 \uce74\ud14c\uace0\ub9ac\ub97c \ub3cc\uba74\uc11c \ud574\ub2f9 \uce74\ud14c\uace0\ub9ac\uc758 \ube14\ub85c\uadf8 \uae00\ub4e4\uc744 \uc218\uc9d1\ud574\uc11c \uc800\uc7a5\ud55c\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "page_info_re = re.compile(\"http://blog\\.naver\\.com/(?P<user_id>.*?)\\?Redirect=Log&logNo=(?P<post_id>.*?)&\")\n",
      "page_info_re2 = re.compile(\"http://(?P<user_id>.*?)\\.blog\\.me/(?P<post_id>.*?)$\")\n",
      "\n",
      "# \ubaa8\ub4e0 \uc8fc\uc81c\ub97c \ub2e4\ud558\uae30\uc5d0\ub294 \ud604\uc7ac \uc11c\ube44\uc2a4 \uc131\ub2a5\uc0c1 \uc81c\uc57d\uc774 \uc788\uc5b4\uc11c \uc5ec\uae30\uc11c\ub294 \uc544\ub798 2\uac00\uc9c0 \ubd84\ub958\ub9cc \ud559\uc2b5 \ubc0f \uc131\ub2a5\ud3c9\uac00\ub97c \ud55c\ub2e4.\n",
      "_SELECT_CATE_LIST = [u'IT\u00b7\ucef4\ud4e8\ud130',u'\uc0ac\ud68c\u00b7\uc815\uce58']\n",
      "\n",
      "cate_info_dict_list = []\n",
      "data_dict = {}\n",
      "for cate in blog_cate_list:\n",
      "    name= cate.select(\".//text()\").extract()[0]\n",
      "    if not name in _SELECT_CATE_LIST:\n",
      "        continue    \n",
      "    \n",
      "    \n",
      "    url= 'http://section.blog.naver.com/'+cate.select(\".//@href\").extract()[0]\n",
      "    data_dict[name] = []\n",
      "    print name\n",
      "    print url\n",
      "    cate_info_dict_list.append({'name':name,'url':url})\n",
      "    # \ub9ce\uc740 \ubb38\uc11c\ub97c \uac00\uc9c0\uace0 \ud14c\uc2a4\ud2b8\ub97c \ubabb\ud558\uae30 \ub54c\ubb38\uc5d0 10page \uae4c\uc9c0 100\uac1c \uae00\ub9cc \uac00\uc838\uc628\ub2e4.\n",
      "    for page in range(1,2):\n",
      "        print page\n",
      "        print url\n",
      "        \n",
      "        page_url = url+\"&option.page.currentPage=%d\"%(page)\n",
      "        print name, page_url\n",
      "        url_xpath = \".//*[@class='template_briefContents']/@href\"\n",
      "        hxs = HtmlXPathSelector(text=requests.get(page_url).text)\n",
      "        url_list = map(lambda i : i.extract(),hxs.select(url_xpath))\n",
      "        \n",
      "        for page_url in url_list:\n",
      "            if not page_url:\n",
      "                continue\n",
      "            data = get_naver_blog_data(page_url)\n",
      "            if not data:\n",
      "                continue\n",
      "            print \"info:\",len(page_url),page_url,\"#\"*10\n",
      "            data_dict[name].append(data.strip())\n",
      "            print len(data)\n",
      "            #print type(main_text)\n",
      "        \n",
      "import pickle\n",
      "filename=\"naver_blog_cate_small_test.pkl\"\n",
      "fout = file(filename,\"w\")\n",
      "pickle.dump(data_dict,fout)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'blog_cate_list' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-6d0749da5ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcate_info_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblog_cate_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".//text()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_SELECT_CATE_LIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'blog_cate_list' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2. \ud559\uc2b5\ub370\uc774\ud130 \ubcc0\ud658"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* \ucef4\ud4e8\ud130\uac00 \ud559\uc2b5\ud560 \uc218 \uc788\ub294 \ud3ec\ub9f7\uc73c\ub85c \ub370\uc774\ud130\ub97c \ubcc0\ud658\ud55c\ub2e4.\n",
      "* \uac01 \ubb38\uc11c\uc5d0 \uc788\ub294 \ubaa8\ub4e0 \ub2e8\uc5b4\ub4e4\uc744 id(=feature) \ub85c \ubcc0\ud658 \n",
      " * word1 - 0, word2 - 1, word3 - 2 .... wordn - n : \uc774\ub7f0\uc2dd\uc73c\ub85c \uac01 \ub2e8\uc5b4\ub9c8\ub2e4 id \ub97c \ubd80\uc5ec \ud55c\ub2e4.\n",
      "* \uac01 \ubb38\uc11c\ub97c \ud558\ub098\uc758 \ubca1\ud130\ub85c \ud45c\ud604\ud55c\ub2e4. \uc774\ub54c \ud574\ub2f9 \ubb38\uc11c\uc5d0 \ud3ec\ud568\ub41c \ub2e8\uc5b4\ub294 1 \uac12\uc744 \uc8fc\uace0, \uc5c6\ub294 \ub2e8\uc5b4\ub294 0 \uac12\uc73c\ub85c \ud45c\ud604\n",
      " * 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 ...... 1 1 0 0 0 0 \n",
      " * \uc774\ub7f0\uc2dd\uc73c\ub85c \ud45c\ud604 \ub418\uac8c \ub41c\ub2e4."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Document Segmentation\n",
      "* \ubb38\uc11c\ub97c \uadf8\ub0e5 \ub744\uc5b4 \uc4f0\uae30\ub97c \uae30\uc900\uc73c\ub85c \ub2e8\uc5b4\ub97c \uad6c\ubd84\ud558\uac8c \ub418\uba74, \uc81c\ub300\ub85c \ubd84\ub9ac \uc548\ub418\ub294 \uacbd\uc6b0\uac00 \ub9ce\ub2e4. \uc608\ub97c \ub4e4\uc5b4\uc11c  \"\uc9d1\uccb4\uad50\uc721\" -> \"\uc9d1\uccb4\uad50\uc721\" \uc774\ub7f0\uc2dd\uc73c\ub85c \ub418\uae30 \ub54c\ubb38\uc5d0 \uc5b8\uc5b4\ucc98\ub9ac\uac00 \ud544\uc694\ud558\ub2e4.\n",
      "* \uc81c\uc77c \uc88b\uc740 \ubc29\ubc95\uc740 Segmentation \uc5d4\uc9c4\uc774 \uc788\uc73c\uba74 \"\uc9d1\uccb4\",\"\uad50\uc721\" \uc774\ub807\uac8c \uc758\ubbf8\ubcc4\ub85c \uad6c\ubd84\uc744 \ud574\uc8fc\uba74 \uc88b\ub2e4.\n",
      "* \ub9cc\uc57d \uc5c6\ub2e4\uba74 bigram \uc774\ub77c\ub294 \ubc29\ubc95\uc744 \uc4f0\uba74 \ub41c\ub2e4. bigram \uc740 \"\uc9d1\uccb4\", \"\uccb4\uad50\",\"\uad50\uc721\" \uc774\ub807\uac8c \uacb9\uccd0\uc11c 2\uac1c\uc529 \ub2e8\uc5b4\ub97c \ub9cc\ub4e4\uc5b4 \uc8fc\ub294 \ubc29\ubc95\uc774\ub2e4. \ube60\ud2b8\ub9ac\ub294 \ub2e8\uc5b4\ub294 \uc5c6\uc9c0\ub9cc, \"\uccb4\uad50\" \ucc98\ub7fc \uad00\ub828 \uc5c6\ub294 \ub2e8\uc5b4\uac00 \ud3ec\ud568\ub420 \uc218\ub294 \uc788\ub2e4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import bigrams\n",
      "def bigram_segment(words):\n",
      "    if type(words)!=unicode:\n",
      "        words=words.decode(\"utf-8\")\n",
      "    token_list=bigrams(words)\n",
      "    return map(lambda i : i[0]+i[1],token_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle,os\n",
      "    return map(lambda i : i[0]+i[1],token_list)\n",
      "#os.system(\"wget http://17.buzzni.com/maestro/naver_blog_cate_select.pkl\")\n",
      "filename=\"naver_blog_cate_select.pkl\"\n",
      "load_data = pickle.load(open(filename))\n",
      "#load_data = data_dict\n",
      "for cate in load_data.keys():\n",
      "    if not cate in _SELECT_CATE_LIST:\n",
      "        load_data.pop(cate)\n",
      "for cate in load_data.keys():\n",
      "    load_data[cate] = load_data[cate][:40]\n",
      "    print cate, len(load_data[cate])\n",
      "cate_list = load_data.keys()\n",
      "cate_dict = dict(zip(cate_list,range(1,len(cate_list)+1)))\n",
      "cate_id_name_dict = dict(zip(range(1,len(cate_list)+1),cate_list))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print cate_dict\n",
      "segment_data_dict = {}\n",
      "word_set = set()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* bigram \uc73c\ub85c segmentation \uc744 \ud558\uace0, word -> id(feature) \uc790\ub8cc\ud615 \uc0dd\uc131."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_list = []\n",
      "for cate in load_data.keys():\n",
      "    segment_data_dict[cate] = []\n",
      "    for data in load_data[cate]:\n",
      "        data_list.append(data)\n",
      "        segmented_data = bigram_segment(data)\n",
      "        word_set.update(set(segmented_data))\n",
      "        segment_data_dict[cate].append(segmented_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'load_data' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-71cfcdf15a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msegment_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_id_dict = dict(zip(list(word_set),range(len(word_set))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = []\n",
      "Y = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* \uac01 \ubb38\uc11c\ub4e4\uc744 word vector \ub85c \ubcc0\ud658\ud55c\ub2e4.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "converted_data_dict = {}\n",
      "\n",
      "for cate in segment_data_dict.keys():\n",
      "    converted_data_dict[cate] = []\n",
      "    for data in segment_data_dict[cate]:\n",
      "        converted_data = np.zeros(len(word_set))\n",
      "        for word in data:\n",
      "            converted_data[word_id_dict[word]] = 1 \n",
      "        converted_data_dict[cate].append(converted_data)\n",
      "        X.append(converted_data)\n",
      "        Y.append(cate_dict[cate])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.array(X)\n",
      "y = np.array(Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_set = set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set()?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_set = set([\"ab\", 1, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "set([1, 2, 'ab'])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_set.update( set([\"ab\", 3, 4, 4, 0.2]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "set([3, 1, 2, 'ab', 4, 0.2])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "     3. \ud559\uc2b5 \ubc0f \uc131\ub2a5 \ucd5c\uc801\ud654"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import cross_validation\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.25, random_state=0)\n",
      "\n",
      "print x.shape, X_train.shape, X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = LinearSVC(loss='l2',C=0.10000000000000001).fit(X_train, y_train)\n",
      "clf.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint\n",
      "svc_params = {\n",
      "    'C': np.logspace(-1, 2, 5),\n",
      "}\n",
      "pprint(svc_params)\n",
      "\n",
      "gs_svc = GridSearchCV(LinearSVC(loss='l2'), svc_params, cv=3, n_jobs=-1)\n",
      "\n",
      "%time _ = gs_svc.fit(X, y)\n",
      "\n",
      "print gs_svc.best_params_,gs_svc.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    4. \uc2e4\uc81c \uc751\uc6a9\uc5d0 \uc0ac\uc6a9"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://blog.naver.com/ehdvkf380?Redirect=Log&logNo=20193683920&from=section'\n",
      "s_it = get_naver_blog_data(url)\n",
      "url = 'http://blog.naver.com/saintcomf?Redirect=Log&logNo=20193703548&from=section'\n",
      "s_society = get_naver_blog_data(url)\n",
      "\n",
      "def classify(doc):\n",
      "    X_test = []\n",
      "    converted_data = np.zeros(len(word_set))\n",
      "    for word in bigram_segment(doc):\n",
      "        if not word_id_dict.has_key(word):\n",
      "            continue\n",
      "        converted_data[word_id_dict[word]] = 1 \n",
      "    converted_data_dict[cate].append(converted_data)\n",
      "    X_test.append(converted_data)\n",
      "    print cate_id_name_dict[clf.predict(X_test)[0]]\n",
      "classify(s_it)\n",
      "classify(s_society)\n",
      "\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clustering \uc608\uc81c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans, DBSCAN\n",
      "from numpy.random import RandomState\n",
      "from sklearn.decomposition import PCA\n",
      "import pylab as pl\n",
      "from itertools import cycle\n",
      "from sklearn.datasets import load_svmlight_file\n",
      "# from adme_rpc.src._db import _ADME_DB\n",
      "\n",
      "import pylab as pl\n",
      "def plot_2D(data, target, target_names):\n",
      "    colors = cycle('rgbcmykw')\n",
      "    target_ids = range(len(target_names))\n",
      "    pl.figure()\n",
      "    for i, c, label in zip(target_ids, colors, target_names):\n",
      "        pl.scatter(data[target == i, 0], data[target == i, 1],\n",
      "                   c=c, label=label)\n",
      "    pl.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2 = X\n",
      "pca = PCA(n_components=2, whiten=True).fit(X2)\n",
      "X_pca = pca.transform(X2)\n",
      "\n",
      "rng = RandomState(42)\n",
      "#kmeans = DBSCAN(eps=0.07, min_samples=4, metric='euclidean', random_state=rng)\n",
      "kmeans = KMeans(n_clusters=3, random_state=rng)\n",
      "kmeans.fit(X_pca)\n",
      "#plot_2D(X_pca, kmeans.labels_, [\"c0\", \"c1\",\"c2\",\"c3\",\"c4\",\"c5\"])  \n",
      "plot_2D(X_pca, kmeans.labels_, [\"c0\", \"c1\", \"c3\"])  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_dict ={}\n",
      "total=0\n",
      "correct=0\n",
      "for idx,each in enumerate(kmeans.labels_):\n",
      "    if not cluster_dict.has_key(each):\n",
      "        cluster_dict[each] = []\n",
      "    print idx, each, Y[idx]\n",
      "print correct/float(total)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}