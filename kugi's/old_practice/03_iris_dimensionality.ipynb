{
 "metadata": {
  "name": "03_iris_dimensionality"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dimensionality Reduction and Visualization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dimensionality reduction is the task of deriving a set of new\n",
      "artificial features that is smaller than the original feature\n",
      "set while retaining most of the variance of the original data.\n",
      "Here we'll use a common but powerful dimensionality reduction\n",
      "technique called Principal Component Analysis (PCA).\n",
      "We'll perform PCA on the iris dataset that we saw before:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA is performed using linear combinations of the original features\n",
      "using a truncated Singular Value Decomposition of the matrix X so\n",
      "as to project the data onto a base of the top singular vectors.\n",
      "If the number of retained components is 2 or 3, PCA can be used\n",
      "to visualize the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2, whiten=True).fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once fitted, the pca model exposes the singular vectors in the components_ attribute:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.components_                           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us project the iris dataset along those first two dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pca = pca.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset has been \u201cnormalized\u201d, which means that the data\n",
      "is now centered on both components with unit variance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.round(X_pca.mean(axis=0), decimals=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.round(X_pca.std(axis=0), decimals=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore the samples components do no longer carry any linear correlation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.round(np.corrcoef(X_pca.T), decimals=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize the projection using pylab, but first\n",
      "let's make sure our ipython notebook is in pylab inline mode"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can visualize the results using the following utility function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "from itertools import cycle\n",
      "\n",
      "def plot_PCA_2D(data, target, target_names):\n",
      "    colors = cycle('rgbcmykw')\n",
      "    target_ids = range(len(target_names))\n",
      "    pl.figure()\n",
      "    for i, c, label in zip(target_ids, colors, target_names):\n",
      "        pl.scatter(data[target == i, 0], data[target == i, 1],\n",
      "                   c=c, label=label)\n",
      "    pl.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now calling this function for our data, we see the plot:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_PCA_2D(X_pca, iris.target, iris.target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that this projection was determined *without* any information about the\n",
      "labels (represented by the colors): this is the sense in which the learning\n",
      "is unsupervised.  Nevertheless, we see that the projection gives us insight\n",
      "into the distribution of the different flowers in parameter space: notably,\n",
      "*iris setosa* is much more distinct than the other two species."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note also that the default implementation of PCA computes the SVD of the full\n",
      "data matrix, which is not scalable when both ``n_samples`` and\n",
      "``n_features`` are big (more that a few thousands).\n",
      "If you are interested in a number of components that is much\n",
      "smaller than both ``n_samples`` and ``n_features``, consider using\n",
      ":class:`sklearn.decomposition.RandomizedPCA` instead."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repeat the above dimensionality reduction with\n",
      "``sklearn.decomposition.RandomizedPCA``.\n",
      "\n",
      "You can re-use the ``plot_PCA_2D`` function from above.\n",
      "Are the results similar to those from standard PCA?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import RandomizedPCA\n",
      "#apply randomized PCA to the iris data as above, and plot the result."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}